{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5477d17-dcb0-46d2-aed3-c6252852375d",
   "metadata": {},
   "source": [
    "### Downloading and Importing Necessary Libraries ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857f0d6e-9eef-41d9-bf51-340f635c8b76",
   "metadata": {},
   "source": [
    "# Data Analytics Job Market Analysis - Web Scraping & Insights\n",
    "\n",
    "## Overview  \n",
    "This project focuses on **web scraping job listings** for Data Analytics roles using **Selenium**. I extracted job titles, company names, locations, job types, and salary ranges from SimplyHired to analyze salary distributions and hiring trends.\n",
    "\n",
    "## Challenges Faced & Fixes  \n",
    "\n",
    "1. **Website Blocking & CAPTCHAs**  \n",
    "   - Some sites detected automation, so I switched to a scrape-friendly job portal.  \n",
    "\n",
    "2. **Dynamic Page Loading Issues**  \n",
    "   - Used `WebDriverWait` to ensure elements were fully loaded before extraction.  \n",
    "\n",
    "3. **Data Extraction Errors**  \n",
    "   - **Company Names Missing**: Adjusted XPath selectors to correctly extract employer names.  \n",
    "   - **Job Location Extraction**: Ensured the right HTML elements were targeted to capture city/state info.  \n",
    "   - **Salary Formatting Issues**:  \n",
    "     - Converted \"95.8K\" to \"95,800\" to ensure accurate numerical representation.  \n",
    "     - Removed \"Estimated:\" text to extract clean salary values.  \n",
    "\n",
    "4. **Inconsistent Job Details Structure**  \n",
    "   - Some job postings did not list salaries, so missing values were handled as \"Not Provided.\"  \n",
    "   - Job type (Full-time/Part-time) was mixed with salary details, so I split them into separate columns.  \n",
    "\n",
    "5. **Data Cleaning & Visualization Issues**  \n",
    "   - Filtered out unrealistic salary values (e.g., below $20,000).  \n",
    "   - Adjusted bin sizes in histograms to avoid distorted distributions.  \n",
    "   - Ensured company analysis only included valid employers (not \"Not Listed\").  \n",
    "\n",
    "## Key Analyses Conducted  \n",
    "\n",
    "- **Top Hiring Companies** – Identifying companies with the highest number of job postings.  \n",
    "- **Salary Distribution** – Analyzing the common salary ranges in Data Analytics roles.  \n",
    "- **Highest-Paying Companies** – Determining which firms offer the best max salaries.  \n",
    "\n",
    "This project provides a structured analysis of the Data Analytics job market, offering insights into salary expectations and hiring trends.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e29ffcc2-93c1-4224-976c-87ea64575f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (4.29.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (25.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.9.0)\n",
      "Requirement already satisfied: webdriver_manager in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from webdriver_manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from webdriver_manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from webdriver_manager) (24.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kaanchi gupta\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install webdriver_manager\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b800e-01a2-4e44-b898-0673aad15d21",
   "metadata": {},
   "source": [
    "### Set-Up Selenium Web Driver ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "271c3e34-af69-44f8-87cf-0434573a0b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open SimplyHired\n",
    "driver.get(\"https://www.simplyhired.com/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "# Set wait time\n",
    "wait = WebDriverWait(driver, 15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67259451-9419-4894-9b25-a04aae0f6e3d",
   "metadata": {},
   "source": [
    "### Search for \"Data Analytics\" Jobs ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "506478c6-8f79-4460-ab81-eabf732d1bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the job title input field and enter \"Data Analytics\"\n",
    "search_box = wait.until(EC.presence_of_element_located((By.NAME, \"q\")))\n",
    "search_box.clear()\n",
    "search_box.send_keys(\"Data Analytics\")\n",
    "\n",
    "# Click the search button\n",
    "search_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@data-testid='findJobsSearchSubmit']\")))\n",
    "search_button.click()\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4da5e2-096b-495c-901b-007f4b11bcd0",
   "metadata": {},
   "source": [
    "### Scrape Job Titles & Links (Up to 10 Pages) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81804b42-f076-46c7-a0a4-1a81ecffb10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store job data\n",
    "job_data = {\"Job Title\": [], \"Job Link\": []}\n",
    "\n",
    "# Set page limit\n",
    "page_number = 1\n",
    "max_pages = 10\n",
    "\n",
    "while page_number <= max_pages:\n",
    "    print(f\"Scraping page {page_number}...\")\n",
    "\n",
    "    job_cards = driver.find_elements(By.XPATH, \"//div[@data-testid='searchSerpJob']\")\n",
    "\n",
    "    if not job_cards:\n",
    "        print(\"No more job listings found. Stopping scraper.\")\n",
    "        break\n",
    "\n",
    "    for job in job_cards:\n",
    "        try:\n",
    "            # Extract Job Title\n",
    "            title = job.find_element(By.XPATH, \".//h2[@data-testid='searchSerpJobTitle']\").text\n",
    "\n",
    "            # Extract Job Link\n",
    "            link_element = job.find_element(By.XPATH, \".//a\")\n",
    "            job_link = link_element.get_attribute(\"href\")\n",
    "\n",
    "            # Store data\n",
    "            job_data[\"Job Title\"].append(title)\n",
    "            job_data[\"Job Link\"].append(job_link)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting job: {e}\")\n",
    "\n",
    "    # Click 'Next Page' if available\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, \"//a[contains(@aria-label, 'Next')]\")\n",
    "\n",
    "        if not next_button.is_displayed() or \"disabled\" in next_button.get_attribute(\"class\"):\n",
    "            print(f\"End of pagination reached. Total jobs extracted: {len(job_data['Job Title'])}\")\n",
    "            break\n",
    "\n",
    "        next_button.click()\n",
    "        time.sleep(5)\n",
    "        page_number += 1\n",
    "\n",
    "    except Exception:\n",
    "        print(f\"Scraping complete. Total jobs extracted: {len(job_data['Job Title'])}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965d6725-8ff9-432f-9c90-8b3094381511",
   "metadata": {},
   "source": [
    "### Convert Job Titles & Links to DataFrame ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "525be35a-d8a2-4a2b-a8e1-8616f785743b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total job listings in the DataFrame: 200\n"
     ]
    }
   ],
   "source": [
    "# Convert scraped job data to DataFrame\n",
    "df = pd.DataFrame(job_data)\n",
    "\n",
    "# Display first 5 rows\n",
    "df.head()\n",
    "\n",
    "print(f\"Total job listings in the DataFrame: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61031d28-b9fb-445d-8272-93907cd8cbe6",
   "metadata": {},
   "source": [
    "### Limit Dataframe to 50 Jobs ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b82190ed-865c-4f6d-bbfe-70408ef87521",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limited = df.head(50)  # Select only the first 100 jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e7fc87-4b83-45d1-ad85-50da5b59e45a",
   "metadata": {},
   "source": [
    " ### Extract Additional Details (for 100 Jobs) and Convert to CSV ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad9ce1-e8b2-48d1-bfcf-688808f82596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kaanchi Gupta\\AppData\\Local\\Temp\\ipykernel_34908\\1426886376.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_limited[\"Company Name\"] = \"\"\n",
      "C:\\Users\\Kaanchi Gupta\\AppData\\Local\\Temp\\ipykernel_34908\\1426886376.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_limited[\"Location\"] = \"\"\n",
      "C:\\Users\\Kaanchi Gupta\\AppData\\Local\\Temp\\ipykernel_34908\\1426886376.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_limited[\"Expected Pay\"] = \"\"\n",
      "C:\\Users\\Kaanchi Gupta\\AppData\\Local\\Temp\\ipykernel_34908\\1426886376.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_limited[\"Job Description\"] = \"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting additional job details...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Add new columns for extracted details\n",
    "df_limited[\"Company Name\"] = \"\"\n",
    "df_limited[\"Location\"] = \"\"\n",
    "df_limited[\"Expected Pay\"] = \"\"\n",
    "df_limited[\"Job Description\"] = \"\"\n",
    "\n",
    "# Step 2: Set up Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "print(\"\\nExtracting additional job details...\")\n",
    "\n",
    "# Step 3: Loop through the first 50 job links\n",
    "for index, row in df_limited.iterrows():\n",
    "    job_link = row[\"Job Link\"]\n",
    "    try:\n",
    "        driver.get(job_link)\n",
    "        time.sleep(random.uniform(5, 10))  # Random delay to avoid detection\n",
    "\n",
    "        # Extract Company Name\n",
    "        try:\n",
    "            company = driver.find_element(By.XPATH, \"//span[@data-testid='detailText']\").text\n",
    "        except:\n",
    "            company = \"Not Listed\"\n",
    "\n",
    "        # Extract Location (Updated XPath)\n",
    "        try:\n",
    "            location = driver.find_element(By.XPATH, \"//span[@data-testid='detailText' and ancestor::span[@data-testid='viewJobCompanyLocation']]\").text\n",
    "        except:\n",
    "            location = \"Not Listed\"\n",
    "\n",
    "        # Extract Expected Pay (Salary)\n",
    "        try:\n",
    "            salary = driver.find_element(By.XPATH, \"//span[@data-testid='detailText'][preceding::div[@data-testid='viewJobBodyJobCompensation']]\").text\n",
    "        except:\n",
    "            salary = \"Not Provided\"\n",
    "\n",
    "        # Extract Job Description (First 500 characters)\n",
    "        try:\n",
    "            job_desc = driver.find_element(By.XPATH, \"//div[@data-testid='viewJobBodyJobDetailsContainer']\").text[:500]\n",
    "        except:\n",
    "            job_desc = \"Description Not Available\"\n",
    "\n",
    "        # Store extracted details\n",
    "        df_limited.at[index, \"Company Name\"] = company\n",
    "        df_limited.at[index, \"Location\"] = location\n",
    "        df_limited.at[index, \"Expected Pay\"] = salary\n",
    "        df_limited.at[index, \"Job Description\"] = job_desc\n",
    "\n",
    "        print(f\"Extracted details for: {row['Job Title']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting details for {job_link}: {e}\")\n",
    "\n",
    "# Step 4: Save the Updated Data to CSV\n",
    "csv_filename = \"data_analytics_jobs_50_fixed.csv\"\n",
    "df_limited.to_csv(csv_filename, index=False)\n",
    "print(f\"\\nData saved to {csv_filename}\")\n",
    "\n",
    "# Step 5: Close the Browser\n",
    "driver.quit()\n",
    "\n",
    "# Step 6: Display the Top 10 Rows\n",
    "df_limited.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a9ce11-6604-4f3f-a983-5fb0a42663da",
   "metadata": {},
   "source": [
    "### Split Job Details to Show 'Job Type' and 'Expected Pay' Separately ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d2e0e6-4efe-4697-8fed-af60f92f6d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split Job Description into Job Type and Expected Pay\n",
    "df_limited[\"Job Type\"] = df_limited[\"Job Description\"].apply(lambda x: x.split(\"\\n\")[0] if isinstance(x, str) else \"Not Listed\")\n",
    "df_limited[\"Expected Pay\"] = df_limited[\"Job Description\"].apply(lambda x: x.split(\"\\n\")[1] if isinstance(x, str) and \"\\n\" in x else \"Not Provided\")\n",
    "\n",
    "# Step 2: Drop the old \"Job Description\" column\n",
    "df_limited.drop(columns=[\"Job Description\"], inplace=True)\n",
    "\n",
    "# Step 3: Save the cleaned data to CSV\n",
    "csv_filename = \"data_analytics_jobs_cleaned.csv\"\n",
    "df_limited.to_csv(csv_filename, index=False)\n",
    "\n",
    "# Step 4: Display the first 10 rows\n",
    "df_limited.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9467dd-8e90-4618-bc84-6eb5e46e634f",
   "metadata": {},
   "source": [
    "### Load the CSV for Data Analysis ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc91b61-2a90-4b53-a174-6a12db1ffa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the cleaned CSV\n",
    "df = pd.read_csv(\"data_analytics_jobs_cleaned.csv\")  \n",
    "\n",
    "# Display first 5 rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3717712e-e58d-40c5-b35d-a1ca65cf7cb1",
   "metadata": {},
   "source": [
    "### Top Hiring Companies ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b9cf06-3d80-4402-a9e2-bddfd159d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out \"Not Listed\" company names\n",
    "filtered_companies = df[df[\"Company Name\"] != \"Not Listed\"]\n",
    "\n",
    "# Count job postings by company (after filtering)\n",
    "top_companies = filtered_companies[\"Company Name\"].value_counts().head(10)\n",
    "\n",
    "# Check if we have enough data to plot\n",
    "if not top_companies.empty:\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(10,5))\n",
    "    top_companies.plot(kind=\"bar\", color=\"cornflowerblue\")\n",
    "    plt.xlabel(\"Company Name\")\n",
    "    plt.ylabel(\"Number of Job Postings\")\n",
    "    plt.title(\"Top 10 Hiring Companies for Data Analytics Jobs\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid company names available for analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97fdd0f-d4f3-4f3e-be8a-0be89d0a0de1",
   "metadata": {},
   "source": [
    "### Extracting Maximum and Minimum Salaries ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7db539-4c6b-46c2-be43-ba60db6798c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and extract salary values\n",
    "def extract_salary(salary):\n",
    "    if pd.isna(salary):  # Check for NaN values\n",
    "        return [None, None]\n",
    "\n",
    "    salary = str(salary).replace(\"Estimated:\", \"\").strip()  # Remove \"Estimated\"\n",
    "\n",
    "    # Extract numeric values (handling \"K\")\n",
    "    numbers = re.findall(r\"(\\d{1,3}(?:\\.\\d)?)[K]?\", salary)  # Capture numbers with optional \"K\"\n",
    "    \n",
    "    # Convert \"K\" values to full numbers\n",
    "    clean_numbers = []\n",
    "    for num in numbers:\n",
    "        if \"K\" in salary:  # If \"K\" is in text, multiply by 1,000\n",
    "            clean_numbers.append(float(num) * 1000)\n",
    "        else:\n",
    "            clean_numbers.append(float(num))\n",
    "    \n",
    "    # Ensure we return exactly two values (min & max salary)\n",
    "    return clean_numbers if len(clean_numbers) == 2 else [None, None]\n",
    "\n",
    "# Apply extraction\n",
    "df[[\"Min Salary\", \"Max Salary\"]] = df[\"Expected Pay\"].apply(lambda x: pd.Series(extract_salary(x)))\n",
    "\n",
    "# Convert to numeric values\n",
    "df[\"Min Salary\"] = pd.to_numeric(df[\"Min Salary\"], errors=\"coerce\")\n",
    "df[\"Max Salary\"] = pd.to_numeric(df[\"Max Salary\"], errors=\"coerce\")\n",
    "\n",
    "# Display first few rows to confirm the fix\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d03ae83-fd76-4796-9362-35b127a1b0a5",
   "metadata": {},
   "source": [
    "### Distribution of Maximum Salaries ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb675ec9-447f-40ae-8faa-2f29b4efda47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop NaN values from Max Salary\n",
    "salary_data = df.dropna(subset=[\"Max Salary\"])\n",
    "\n",
    "# Filter out unrealistic values (e.g., below $20,000)\n",
    "salary_data = salary_data[salary_data[\"Max Salary\"] > 20000]\n",
    "\n",
    "# Define bins dynamically based on data range\n",
    "bins = np.linspace(salary_data[\"Max Salary\"].min(), salary_data[\"Max Salary\"].max(), 15)  # 15 bins\n",
    "\n",
    "# Plot histogram for Max Salary only\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(salary_data[\"Max Salary\"], bins=bins, alpha=0.7, color=\"red\", edgecolor=\"black\")\n",
    "plt.xlabel(\"Max Salary (USD)\")\n",
    "plt.ylabel(\"Number of Jobs\")\n",
    "plt.title(\"Distribution of Max Salaries for Data Analytics Jobs\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adbb904-17f5-4bd1-b66a-6adfd726d51a",
   "metadata": {},
   "source": [
    "### Top 10 Companies by Average Max Salary ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82b2cc3-72d8-448e-8532-0e1068356fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter out \"Not Listed\" companies\n",
    "filtered_data = df[df[\"Company Name\"] != \"Not Listed\"]\n",
    "\n",
    "# Group by company and calculate the average max salary\n",
    "company_salary = filtered_data.groupby(\"Company Name\")[\"Max Salary\"].mean().sort_values(ascending=False).head(10)\n",
    "\n",
    "# Check if we have enough data to plot\n",
    "if not company_salary.empty:\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(10,5))\n",
    "    company_salary.plot(kind=\"bar\", color=\"green\", edgecolor=\"black\")\n",
    "    plt.xlabel(\"Company Name\")\n",
    "    plt.ylabel(\"Average Max Salary (USD)\")\n",
    "    plt.title(\"Top 10 Companies by Average Max Salary for Data Analytics Jobs\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid company salary data available for analysis.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
